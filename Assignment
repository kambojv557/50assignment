------ Question -1-------
[root@ip-172-31-42-126 04-controllers]# kubectl label nodes ip-172-31-38-216.ap-southeast-1.compute.internal disk=ssd
node/ip-172-31-38-216.ap-southeast-1.compute.internal labeled
[root@ip-172-31-42-126 04-controllers]# kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   1         1         0       1            0           disk=ssd        4d1h
If we have 100 nodes then it allocate 1 pods in every nodes so that cpu utilisation of every nodes is not high for rc it can not able to allocate each pods in 1 nodes 
daemon set will run on every node , it is hardware based , if nodes are 100 then daemon set automatically replicate its value as 100 and it is used to monitor the nodes 


--------Question -2 -----------------
We can run this command for zero downtime.
kubectl patch deployment kubia -p '{"spec": {"minReadySeconds": 10}}'   ---- after we run this command old pod got terminated after 10 sec and new pod is running 
for update the container image we have to run this command :- kubectl set image deployment kubia nodejs=luksa



----------Question -4-------
-1st we delete VOTE pod then Observe what happens both in frontEnd & in Unix
[root@ip-172-31-42-126 ~]# kubectl get po
NAME                                   READY   STATUS    RESTARTS   AGE
db-b54cd94f4-54knq                     1/1     Running   0          22h
firstapp-firstchart-86bd46c54f-dgl9z   1/1     Running   0          151m
redis-868d64d78-rgvdf                  1/1     Running   0          22h
result-5d57b59f4b-hl7gh                1/1     Running   0          22h
vote-94849dc97-9hmst                   1/1     Running   0          9s
worker-dd46d7584-x2r88                 1/1     Running   0          22h


in frontend it works fine , there is no impact  when we delete voter pod , seemless transaction done from the old pod to new pod , in unix  old pod get deleted new vote pod created with new name but that didnt impact anything .

2nd we delete the WORKER pod  then Observe what happens both in frontEnd & in Unix

2nd we delete the WORKER pod  then Observe what happens both in frontEnd & in Unix


[root@ip-172-31-42-126 ~]# kubectl logs worker-dd46d7584-x2r88
Waiting for db
Waiting for db
Waiting for db
Connected to db
Found redis at 10.108.1.221
Connecting to redis
Processing vote for 'a' by '10fa95eaab68509'
Processing vote for 'a' by '9ee062568efbf2e'
Processing vote for 'b' by '2a8eb112d57578b'
Processing vote for 'b' by 'd5152da8cefeb3a'
Processing vote for 'b' by '2b40a2fcf47e0f'
Processing vote for 'a' by '9ee062568efbf2e'
Processing vote for 'a' by '9ee062568efbf2e'

[root@ip-172-31-42-126 ~]# kubectl delete po worker-dd46d7584-x2r88
pod "worker-dd46d7584-x2r88" deleted


[root@ip-172-31-42-126 ~]# kubectl get po
NAME                                   READY   STATUS    RESTARTS   AGE
db-b54cd94f4-54knq                     1/1     Running   0          23h
firstapp-firstchart-86bd46c54f-dgl9z   1/1     Running   0          3h8m
redis-868d64d78-rgvdf                  1/1     Running   0          23h
result-5d57b59f4b-hl7gh                1/1     Running   0          23h
vote-94849dc97-9hmst                   1/1     Running   0          37m
worker-dd46d7584-gx7sb                 1/1     Running   0          46s

[root@ip-172-31-42-126 ~]# kubectl logs worker-dd46d7584-gx7sb
Connected to db
Found redis at 10.108.1.221
Connecting to redis


logs are deleted in unix 
but nothing happened to data end 

3rd once  DB pod deletd  Observe what happens both in frontEnd & in Unix
[root@ip-172-31-42-126 ~]# kubectl get po
NAME                                   READY   STATUS        RESTARTS   AGE
db-b54cd94f4-4wtdx                     1/1     Running       0          10s
db-b54cd94f4-54knq                     1/1     Terminating   0          23h
firstapp-firstchart-86bd46c54f-dgl9z   1/1     Running       0          3h11m
redis-868d64d78-rgvdf                  1/1     Running       0          23h
result-5d57b59f4b-hl7gh                1/1     Running       0          23h
vote-94849dc97-9hmst                   1/1     Running       0          40m
worker-dd46d7584-gx7sb                 1/1     Running       0          3m16s


[root@ip-172-31-42-126 ~]# kubectl get po
NAME                                   READY   STATUS    RESTARTS   AGE
db-b54cd94f4-4wtdx                     1/1     Running   0          76s
firstapp-firstchart-86bd46c54f-dgl9z   1/1     Running   0          3h12m
redis-868d64d78-rgvdf                  1/1     Running   0          23h
result-5d57b59f4b-hl7gh                1/1     Running   1          23h
vote-94849dc97-9hmst                   1/1     Running   0          41m
worker-dd46d7584-gx7sb                 1/1     Running   1          4m22s

in front end  all data is gone , it is showing no votes yet

in unix end All the data gone and it is showing no vote in unix side worker pod and result pod restarted as there is socket connection between those pod , when db pod is deleted worker pods try to pushing the data to db worker once to write the data into database , and database is not there so he is restarting the container so that connection would be there and same for result pod db pod is pulling the data to result pod and once db pod is deleted result pod is trying to access the db pod and database is not there so it is restarting so that connection would be there between result and db pods 
once both container is restarted now they are connecting to new database 
